{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mage\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mage\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mage\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"c:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"c:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"c:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\spacy\\compat.py\", line 39, in <module>\n",
      "    from thinc.api import Optimizer  # noqa: F401\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\thinc\\api.py\", line 1, in <module>\n",
      "    from .backends import (\n",
      "  File \"c:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\thinc\\backends\\__init__.py\", line 17, in <module>\n",
      "    from .cupy_ops import CupyOps\n",
      "  File \"c:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\thinc\\backends\\cupy_ops.py\", line 16, in <module>\n",
      "    from .numpy_ops import NumpyOps\n",
      "ImportError: DLL load failed while importing numpy_ops: The specified module could not be found.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m spacy download en_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the English language model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mage\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\weightedrank-VABqxEaP-py3.12\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved clean_text function with stemming and negation handling\n",
    "# Function to clean resume text\n",
    "def clean_text(resumeText):\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText)\n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    resumeText = re.sub(r'\\\\[rn]', ' ', resumeText)  # remove \\r and \\n\n",
    "    return resumeText.strip()  # strip leading and trailing whitespace\n",
    "\n",
    "def extract_experience(text):\n",
    "    experience_score = 0\n",
    "    # Initialize spaCy Matcher\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    # Define pattern for matching experience mentions\n",
    "    pattern = [{\"IS_DIGIT\": True}, {\"LOWER\": {\"IN\": [\"to\", \"or\"]}, \"OP\": \"?\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}, {\"LOWER\": {\"IN\": [\"year\", \"yr\", \"yrs\", \"year's\"]}}]\n",
    "    matcher.add(\"EXPERIENCE\", [pattern])\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Iterate over matches found by the Matcher\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        start_token = doc[start]\n",
    "        end_token = doc[end - 1]\n",
    "        start_year = int(start_token.text)\n",
    "        end_year = int(end_token.text) if end_token.text.isdigit() else None\n",
    "        if end_year:\n",
    "            years = end_year - start_year\n",
    "        else:\n",
    "            years = 1  # If only start year mentioned, consider 1 year of experience\n",
    "        # Additional points for relevant titles\n",
    "        title = \"\"\n",
    "        for token in doc[start:end]:\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                title += token.text + \" \"\n",
    "        if title:\n",
    "            experience_score += years * 2\n",
    "        # Weighted based on experience duration\n",
    "        experience_score += years\n",
    "    return experience_score\n",
    "\n",
    "def extract_education(text):\n",
    "    education_score = 0\n",
    "    doc = nlp(text)\n",
    "    # Extract entities related to education\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DEGREE\":\n",
    "            degree_text = ent.text.lower()\n",
    "            if \"bachelor\" in degree_text or \"b.e.\" in degree_text or \"btech\" in degree_text or \"bachelor's\" in degree_text or \"bs\" in degree_text:\n",
    "                education_score += 1\n",
    "            elif \"master\" in degree_text or \"mphil\" in degree_text or \"master's\" in degree_text:\n",
    "                education_score += 2\n",
    "            elif \"phd\" in degree_text or \"Ph.d\" in degree_text:\n",
    "                education_score += 3\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            education_score += 1  # Points for attending a university\n",
    "    return education_score\n",
    "\n",
    "# Function to calculate keyword score\n",
    "def calculate_keyword_score(text, keywords, weights):\n",
    "  score = 0\n",
    "  for word, weight in zip(keywords, weights):\n",
    "    score += text.count(word) * weight\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a Pandas dataframe\n",
    "df = pd.read_csv('resumes.csv')\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['id'] = range(1, len(df) + 1)\n",
    "df.set_index('id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean resume text in each row\n",
    "df['cleaned_text'] = df['Resume'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (replace with more sophisticated methods)\n",
    "df['experience_score'] = df['cleaned_text'].apply(extract_experience)\n",
    "df['education_score'] = df['cleaned_text'].apply(extract_education)\n",
    "print(df.head())\n",
    "# Define job-specific keywords and weights\n",
    "keywords = [\"java\", \"spring\", \"hibernate\", \"maven\", \"J2EE\", \"SQL\", \"RESTful\", \"API\", \"JavaScript\", \"HTML\", \"CSS\", \"Git\"]\n",
    "weights = [4, 1, 3, 3, 3, 2, 3, 3, 4, 3, 3, 2]# Adjust weights based on importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate keyword score for each resume\n",
    "df['keyword_score'] = df['cleaned_text'].apply(lambda x: calculate_keyword_score(x, keywords, weights))\n",
    "\n",
    "# Define weights for different factors (adjust based on your needs)\n",
    "weight_experience = 0.2\n",
    "weight_education = 0.7\n",
    "weight_keywords = 0.1\n",
    "#+ (weight_skills * df.apply(lambda x: len(x['skills']), axis=1))\n",
    "# Calculate overall ranking score\n",
    "df['rank_score'] = (weight_experience * df['experience_score']) + (weight_education * df['education_score']) + (weight_keywords * df['keyword_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by ranking score (highest to lowest)\n",
    "df_sorted = df.sort_values(by='rank_score', ascending=False)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "# Assuming df_sorted is your DataFrame containing the 'Resume' and 'rank_score' columns\n",
    "print(df_sorted[['cleaned_text','rank_score']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weightedrank-VABqxEaP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
